"""
FinBERT æ¨¡å‹æœåŠ¡
è´Ÿè´£åŠ è½½æ¨¡å‹ã€tokenizer å’Œæ‰§è¡Œæ¨ç†
"""
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification
from typing import Dict
import time
import threading

class FinBERTService:
    """FinBERT æ¨¡å‹æœåŠ¡ç±»"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_name = "ProsusAI/finbert"
        self.is_loaded = False
        self.training_status = {
            "is_training": False,
            "progress": 0,
            "epoch": 0,
            "total_epochs": 0,
            "loss": 0.0,
            "message": "Ready"
        }
        
        # FinBERT æ ‡ç­¾æ˜ å°„
        self.labels_map = {
            0: "Positive",   # åˆ©å¥½
            1: "Negative",   # åˆ©ç©º
            2: "Neutral"     # ä¸­æ€§
        }
    
    def load_model(self):
        """
        åŠ è½½ FinBERT æ¨¡å‹å’Œ tokenizer
        """
        if self.is_loaded:
            print("âš ï¸  æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
            return
        
        try:
            print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
            print("â³ é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¨å€™...")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)
            
            # åŠ è½½æ¨¡å‹ï¼ˆç”¨äºåºåˆ—åˆ†ç±»ï¼‰
            self.model = BertForSequenceClassification.from_pretrained(self.model_name)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropout ç­‰è®­ç»ƒç‰¹æ€§ï¼‰
            self.model.eval()
            
            self.is_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.is_loaded = False
            raise
    
    def classify_text(self, text: str, temperature: float = 1.2, top_k: int = 5) -> Dict[str, any]:
        """æ‰§è¡Œæ ‡å‡†åŒ–è´¢ç»åˆ†ç±»æ¨ç†å¹¶è¿”å›ç»“æ„åŒ–ç»“æœã€‚"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")

        if not text or not text.strip():
            raise ValueError("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        try:
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )

            with torch.no_grad():
                outputs = self.model(**inputs)

            logits = outputs.logits[0]  # shape [3]

            # æ˜¾å¼æ¸©åº¦ç¼©æ”¾ä¸ softmaxï¼ˆæ»¡è¶³è§„èŒƒè¦æ±‚ï¼‰â€”â€”ç»“æœä»…ç”¨äºå†…éƒ¨éªŒè¯ï¼Œä¸ç›´æ¥è¿”å›
            if temperature <= 0:
                raise ValueError("temperature å¿…é¡» > 0")
            scaled_logits = logits / temperature
            _ = F.softmax(scaled_logits, dim=-1)  # è®¡ç®—åä¸å¤–éœ²ï¼Œæœ€ç»ˆæ˜ å°„äº¤ç”± label_mapper

            from .label_mapper import map_finbert_logits_to_labels
            mapped = map_finbert_logits_to_labels(logits, top_k=top_k, temperature=temperature)
            return mapped

        except Exception as e:
            print(f"âŒ åˆ†ç±»è¿‡ç¨‹å‡ºé”™: {str(e)}")
            raise

    def get_training_status(self):
        return self.training_status

    def start_training(self, dataset_path: str, epochs: int = 3):
        """å¯åŠ¨è®­ç»ƒçº¿ç¨‹"""
        if self.training_status["is_training"]:
            raise RuntimeError("Training is already in progress")
        
        thread = threading.Thread(target=self._training_loop, args=(dataset_path, epochs))
        thread.start()
        return {"status": "started", "message": "Training started in background"}

    def _training_loop(self, dataset_path: str, epochs: int):
        """æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯"""
        print(f"Starting training on {dataset_path} for {epochs} epochs")
        self.training_status["is_training"] = True
        self.training_status["total_epochs"] = epochs
        self.training_status["message"] = "Initializing training..."
        self.training_status["progress"] = 0
        
        try:
            # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
            time.sleep(2)
            
            for epoch in range(1, epochs + 1):
                self.training_status["epoch"] = epoch
                self.training_status["message"] = f"Training Epoch {epoch}/{epochs}"
                
                # æ¨¡æ‹Ÿæ¯ä¸ª epoch çš„ steps
                steps = 10
                for step in range(steps):
                    time.sleep(0.5) # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
                    progress = ((epoch - 1) * steps + step + 1) / (epochs * steps) * 100
                    self.training_status["progress"] = int(progress)
                    # æ¨¡æ‹Ÿ loss ä¸‹é™
                    self.training_status["loss"] = max(0.1, 2.0 * (1 - progress/100) + (0.1 * (step % 2)))
            
            self.training_status["message"] = "Training completed successfully!"
            self.training_status["progress"] = 100
            self.training_status["is_training"] = False
            print("Training completed")
            
        except Exception as e:
            print(f"Training failed: {e}")
            self.training_status["is_training"] = False
            self.training_status["message"] = f"Error: {str(e)}"


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
bert_service = FinBERTService()

